{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072e395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFYING IMAGES\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1fb17ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c10214b760>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set tha the color chanel value will be first\n",
    "K.set_image_data_format('channels_last')\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "# set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "# reshape training and test image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], \n",
    "                                height, \n",
    "                                width,\n",
    "                               channels)\n",
    "data_test = data_test.reshape(data_test.shape[0],                            \n",
    "                             height,\n",
    "                             width,\n",
    "                             channels)\n",
    "\n",
    "# rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "# ine-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "# start neural network\n",
    "network = Sequential()\n",
    "# add convolutional layer with 64 filters, a 5x5 window and ReLU \n",
    "network.add(Conv2D(filters=64,\n",
    "                  kernel_size=(5, 5),\n",
    "                  input_shape=(height, width, channels),\n",
    "                  activation='relu'))\n",
    "# add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "# add layer to flatten input\n",
    "network.add(Flatten())\n",
    "# add dense ReLU layer\n",
    "network.add(Dense(units=128,\n",
    "                 activation='relu'))\n",
    "# add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "# add dense softmax layer\n",
    "network.add(Dense(units=number_of_classes,\n",
    "                 activation='softmax'))\n",
    "\n",
    "# compile network\n",
    "network.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# train neural network\n",
    "network.fit(features_train,\n",
    "           target_train,\n",
    "           epochs=2,\n",
    "           verbose=0,\n",
    "           batch_size=1000,\n",
    "           validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf70fe7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_5700/3172927023.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\antch\\AppData\\Local\\Temp/ipykernel_5700/3172927023.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    zoom_range=0.3, # randomly zoom in on images\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Improving Perfomance with Image Augmentation\n",
    "\n",
    "# create image augmentation\n",
    "#augmentation = ImageDataGenerator(featurewise_center=True,# apply ZCA witening\n",
    "                                 zoom_range=0.3, # randomly zoom in on images\n",
    "                                 width_shift_range=0.2, \n",
    "                                 horizontal_flip=True,\n",
    "                                 rotation_range=90)\n",
    "\n",
    "# process all images from the directory 'row/images'\n",
    "#augment_images = augmentation.flow_from_directory('raw/images',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                                 save_to_dir='processed/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFYING TEXT\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "# set number of features\n",
    "number_of_features = 1000\n",
    "# load data and target vector\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "\n",
    "# use padding or truncation to make observation have 400 features\n",
    "features_train = sequence.pad_sequences(data_train, maxlen=400)\n",
    "features_test = sequence.pad_sequences(data_test, maxlen=400)\n",
    "\n",
    "# start neural network\n",
    "network = models.Sequential()\n",
    "# add embedding layer\n",
    "network.add(layers.Embedding(input_dim=number_of_features,\n",
    "                           output_dim=128))\n",
    "# add a long short-term memory layer with 128 units\n",
    "network.add(layers.LSTM(units=128))\n",
    "# add dense sigmoid layer\n",
    "network.add(layers.Dense(units=1,\n",
    "                        activation='sigmoid'))\n",
    "\n",
    "# compile network\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='Adam',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# train neural network\n",
    "history = network.fit(features_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=0,\n",
    "                     batch_size=1000,\n",
    "                     validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884a19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
